{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e07adc",
   "metadata": {},
   "source": [
    "# Week 3 Notebook 3: Data Cleaning & Wrangling\n",
    "\n",
    "In this lesson, we are going to practice our data cleaning and wrangling skills. \n",
    "\n",
    "We will learn how to:\n",
    "- Reshape data by adding and removing columns\n",
    "- Handle missing values by dropping rows and columns\n",
    "- Handle missing values by imputing values\n",
    "- Convert data types\n",
    "- Split data\n",
    "- Create simple data visualisations\n",
    "\n",
    "We will use the same `jobs` data set as the previous lesson (from [Pavan Tinniru on Kaggle](https://www.kaggle.com/pavantanniru/-datacleaningforbeginnerusingpandas)).\n",
    "\n",
    "First, we need to load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jobs = pd.read_csv('Data-cleaning-for-beginners-using-pandas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610a1f6",
   "metadata": {},
   "source": [
    "Take a look at our data set again. There are not so many rows, so let's `print` to show all our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce092959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536d614",
   "metadata": {},
   "source": [
    "We can check the data types using `dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5216c7",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "\n",
    "The first thing we want to do is to remove the redundant column `Index` from our DataFrame, because Pandas has automatically provided an index for the rows. We will save the result in a new `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67936b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using 'drop' method to drop columns\n",
    "jobs_2 = jobs.drop(columns = 'Index')\n",
    "jobs_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1f03f",
   "metadata": {},
   "source": [
    "The original DataFrame `jobs` is not modified by the above command.\n",
    "\n",
    "We can also use the parameter `inplace=True` which will modify the existing DataFrame `jobs`, because the changes will be made \"in-place\" which means the same DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875549de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the DataFrame in-place\n",
    "jobs.drop(columns = 'Index', inplace = True)\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49100f99",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "The original data set also contains missing values. One way to check the number of missing values is to combine two methods `isnull()` and `sum()`.\n",
    "\n",
    "- `isnull()` returns `True` if a value is null and `False` otherwise.\n",
    "- `sum()` adds up the `True` values. `True` evaluates to 1 and `False` evaluates to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c0e6b",
   "metadata": {},
   "source": [
    "We can see that this gives us 7 null values for the `Age` column and 1 null value for the `Rating` column. \n",
    "\n",
    "We can drop rows or columns that contain missing values using `dropna()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355df9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain any null values\n",
    "jobs_dropByRow = jobs.dropna()\n",
    "print(jobs_dropByRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ebb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the argument 'axis=1' to drop columns that contains any null values\n",
    "# (by default the argument is 'axis=0', which means drop by row)\n",
    "jobs_dropByCol = jobs.dropna(axis=1)\n",
    "print(jobs_dropByCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d58563",
   "metadata": {},
   "source": [
    "As you can see, this will just remove the missing values completely, but we might also lose important data from the other rows and columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8728b4d",
   "metadata": {},
   "source": [
    "### Missing Value Imputation\n",
    "\n",
    "A better strategy to treat missing values would be to **impute** it with new values. Usually, this would be an \"average\" value or the most probable value for the particular column.\n",
    "\n",
    "For a column with numerical values, the imputation method is usually the *mean* or *median*. \n",
    "\n",
    "For a column with categorical values, the imputation method is usually the category that occurs most frequently.\n",
    "\n",
    "For example, we could impute the mean age from the data set into any missing `Age` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute by calculating mean into missing values using 'fillna()''\n",
    "jobs['Age'].fillna(jobs['Age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8433503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5a4cb",
   "metadata": {},
   "source": [
    "As you can see, this maintains the total number of rows as 29. However, some of the `Age` values now have the mean value 39.045455. This may not be very meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137b4c9",
   "metadata": {},
   "source": [
    "### Data Type Conversion\n",
    "\n",
    "We can convert the data types of a column using the `astype()` method. For example, let's convert the `Age` column from `float` into `int`. This will just remove the decimal places from the floating-point values as they are converted into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the 'Age column with the values converted to int'\n",
    "jobs['Age'] = jobs['Age'].astype('int')\n",
    "\n",
    "# Check the results\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a43482",
   "metadata": {},
   "source": [
    "### Replacing Values\n",
    "\n",
    "Another useful Pandas method is `replace()`, which will replace existing values with new ones to make sure that the data is consistent. For example, the values in `Location` are not very consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80076724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each value\n",
    "jobs['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a067846",
   "metadata": {},
   "source": [
    "Some of the locations are recorded with a comma and some without. It also seems redundant to store an acronym of the location, such as 'New York, Ny' and 'India, In'. Let's use the `replace()` method.\n",
    "\n",
    "Regular Expressions or Regex are used to match strings. The regular expression `'India.*'` means a string that starts with the letters 'India' followed by any character, any number of times. So, this would help to match `India,In` and `India In`. We are using the argument `regex=True` to indicate that the letters to be replaced are defined using regular expressions.\n",
    "\n",
    "You can read more about [Regular Expressions](https://www.computerhope.com/jargon/r/regex.htm) from Computer Hope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any location starting with the letters 'New York' with just 'New York',\n",
    "# any location starting with the letters 'India' with 'India'\n",
    "# any location starting with the letters 'Australia' with 'Australia'.\n",
    "jobs['Location'].replace('India.*', 'India', regex = True, inplace = True)\n",
    "jobs['Location'].replace('New York.*', 'New York', regex = True, inplace = True)\n",
    "jobs['Location'].replace('Australia.*', 'Australia', regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448525e",
   "metadata": {},
   "source": [
    "### Replacing with Lists\n",
    "\n",
    "We can also notice that the values in `Easy Apply` are either 'TRUE' or '-1'. Let's check by counting each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['Easy Apply'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14fac8",
   "metadata": {},
   "source": [
    "We can use the `replace()` method to replace the value '-1' with the Boolean value `False` and the string value 'TRUE' with the Boolean value `True`. We can pass both values to be replaced and the new values as List into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in 'Easy Apply' column\n",
    "# -1 should be replaced with False and 'TRUE' should be replaced with True.\n",
    "jobs['Easy Apply'].replace(['-1','TRUE'],[False, True], inplace = True)\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eeb1c7",
   "metadata": {},
   "source": [
    "Check the data type now to ensure the column stores Boolean (bool) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5c197",
   "metadata": {},
   "source": [
    "### Replacing with Null Values\n",
    "\n",
    "We can also replace values with `NaN`, which means 'Not a Number', null, or missing value. Although we dropped or imputed missing values earlier, sometimes it might be better to keep the value as missing rather than an invalid value.\n",
    "\n",
    "For example, there are several values in the `Rating` column and `Established` column which are -1. Keeping the value as -1 might cause calculations or plotting to be not meaningful.\n",
    "\n",
    "We can use the `numpy` library to obtain the `NaN` value as `numpy` is usually used for numerical calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "jobs['Rating'].replace(-1,np.NaN, inplace = True )\n",
    "jobs['Established'].replace(-1, np.NaN, inplace = True)\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a874a",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "\n",
    "Another common data wrangling task would be to split data into more than one column, especially if there has been columns that records multiple string.\n",
    "\n",
    "For example, the `Salary` column contains a string with a lower and upper bound for salary values. However, it might be more useful if the salary was a numeric value so that we could calculate the *mean* or *median* salary.\n",
    "\n",
    "We can use the string `split()` function for this by specifying *where* we want to split the string.\n",
    "The `expand` argument also indicates that we want to expand the values into more than one column.\n",
    "\n",
    "Now, let's split the `Salary` column into two new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86893fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Salary' and add two new columns\n",
    "jobs[['Min Salary', 'Max Salary']] = jobs['Salary'].str.split('-', n = 1, expand = True)\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb031",
   "metadata": {},
   "source": [
    "Check the data type of the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29226b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fddd5",
   "metadata": {},
   "source": [
    "Since `Min Salary` and `Max Salary` are of type `object`, let's convert them into `integer` values. We can use the `astype()` method, but before that let's strip the 'k' and '$' characters from the right and left sides respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the special characters from 'Min Salary' and convert it to an integer\n",
    "jobs['Min Salary'] = jobs['Min Salary'].str.rstrip('k')\n",
    "jobs['Min Salary'] = jobs['Min Salary'].str.lstrip('$')\n",
    "jobs['Min Salary'] = jobs['Min Salary'].astype(int)\n",
    "\n",
    "# Do the same for 'Max Salary'\n",
    "jobs['Max Salary'] = jobs['Max Salary'].str.rstrip('k')\n",
    "jobs['Max Salary'] = jobs['Max Salary'].str.lstrip('$')\n",
    "jobs['Max Salary'] = jobs['Max Salary'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df187853",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd832a",
   "metadata": {},
   "source": [
    "Now, we can calculate the estimated salary to be the midpoint of the `Min Salary` and `Max Salary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate an estimated salary for each job as a new column\n",
    "jobs['Est Salary'] = (jobs['Max Salary'] + jobs['Min Salary'])/2\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2762767",
   "metadata": {},
   "source": [
    "## Plotting with Pandas\n",
    "\n",
    "Now that we have cleaned up the data, we can perform some simple visualisation/plotting with Pandas.\n",
    "\n",
    "For example, let's create a pie chart using `plot.pie`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the value counts for each location\n",
    "jobs1 = jobs['Location'].value_counts()\n",
    "print(jobs1)\n",
    "print(jobs1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb703a",
   "metadata": {},
   "source": [
    "Now that we have counts of the locations, we can plot the values and use the index as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart\n",
    "jobs1.plot.pie(labels = jobs1.index, autopct = '%.1f%%', \n",
    "               colors = [\"lightpink\", \"violet\", \"lightblue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c569c84",
   "metadata": {},
   "source": [
    "### Bar Charts\n",
    "\n",
    "A bar chart can be created in Pandas by grouping the categorical data.\n",
    "\n",
    "For example, we want to find the mean estimated salary by location and whether it is 'Easy Apply'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, find the mean salary grouped by 'Location', then 'Easy Apply' values\n",
    "meanSalary = jobs.groupby(['Location', 'Easy Apply'])['Est Salary'].mean()\n",
    "meanSalary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e06463",
   "metadata": {},
   "source": [
    "This gives us the mean salary grouped by location, then whether it is Easy Apply. \n",
    "\n",
    "We can unstack the group so that there the rows are by Location, but 'Easy Apply' values are arranged in columns. This returns a DataFrame object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack so that the Easy Apply Values are arranged in columns\n",
    "meanSalaryByLocation = meanSalary.unstack()\n",
    "meanSalaryByLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a907df1",
   "metadata": {},
   "source": [
    "Now, we can create a simple bar plot to show the mean salaries, which are the values in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart \n",
    "meanSalaryByLocation.plot.bar(ylabel='Mean Estimated Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d5fee",
   "metadata": {},
   "source": [
    "### Stacked Bar Chart\n",
    "\n",
    "Let's try to create a stacked bar chart to show how many jobs are being counted. We can use the same `groupby`, but this time we use the `count()` function to count the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows by Location and Easy Apply\n",
    "countLocation = jobs.groupby(['Location', 'Easy Apply'])['Est Salary'].count()\n",
    "countLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9077132",
   "metadata": {},
   "source": [
    "Similarly, we can unstack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "countByLocation = countLocation.unstack()\n",
    "countByLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75a8d8",
   "metadata": {},
   "source": [
    "We can stack the values so that it shows the total count of jobs by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacked bar chart\n",
    "countByLocation.plot.bar(stacked = True, ylabel = 'Number of Jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b49e46",
   "metadata": {},
   "source": [
    "### Scatter Plot\n",
    "\n",
    "Now that we have the 'Estimated Salary' as a numeric value, we can also create a quick scatter plot with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a113edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of Estimated Salary vs Rating\n",
    "jobs.plot.scatter(x = 'Rating', y = 'Est Salary', marker = '*', s = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98d8ca",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The Bike Sharing Data Set contains data about bike rentals from Capital Bikeshare on the 1st of December 2021. The data is adapted from [Capital Bike Share's System Data](https://www.capitalbikeshare.com/system-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58765076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the data set into a DataFrame called 'bikes'\n",
    "bikes = pd.read_csv(\"bikeshare.csv\")\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54880df",
   "metadata": {},
   "source": [
    "**Q1 Check Data Types**\n",
    "\n",
    "Check the data types for each of the columns of the `bikes` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73391708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89a483",
   "metadata": {},
   "source": [
    "**Q2 Dropping Columns**\n",
    "\n",
    "Drop the columns `ride_id`, `start_station_name` and `end_station_name` and save the result in a new object called `newbikes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33217223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a241f",
   "metadata": {},
   "source": [
    "**Q3 Count Missing Values**\n",
    "\n",
    "Check the number of missing values for each column in the `bikes` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d48f3",
   "metadata": {},
   "source": [
    "**Q4 Drop Missing Values**\n",
    "\n",
    "Since there are over 7000 rows of data, drop all the rows that contain any null values by using the `inplace` parameter to update the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43221b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097acff",
   "metadata": {},
   "source": [
    "**Q5 Convert to DateTime Type**\n",
    "\n",
    "The starting time of the bike rental is interpreted by Python as a string. We should convert it using the Pandas method `to_datetime()` as follows:<br>\n",
    "`bikes['started_at'] = pd.to_datetime(bikes['started_at'])`\n",
    "\n",
    "Write the statement above and also convert the `ended_at` column to a `datetime` data type. Then run `bikes.dtypes` again to check the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbe632",
   "metadata": {},
   "source": [
    "**Q6 Add Column**\n",
    "\n",
    "Add a column called `duration`, which is the difference between the values in the `ended_at` and `started_at` columns, then check the data types again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73caa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f729728",
   "metadata": {},
   "source": [
    "**Q7 Descriptive Statistics **\n",
    "\n",
    "The duration is a `timedelta` object. Use `describe` to view information about `bikes['duration']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb27922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q7 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0234a",
   "metadata": {},
   "source": [
    "The time delta is given in terms of days, hours, minutes and seconds.\n",
    "We can convert the `timedelta` into seconds using the `DateTime` method `total_seconds()` like this: <br>\n",
    "`bikes['duration'].dt.total_seconds()`.\n",
    "\n",
    "By using this method, we can add a new column called `duration_in_min` that calculates the duration in ***minutes***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb330b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column which is the duration in MINUTES\n",
    "bikes['duration_in_min'] =bikes['duration'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6e0bb",
   "metadata": {},
   "source": [
    "Let's check the value counts of the `rideable_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['rideable_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cdd36",
   "metadata": {},
   "source": [
    "**Q8 Replacing Values**\n",
    "\n",
    "By using the `replace()` method, modify the 'rideable_type' column by changing all values of `classic_bike` into `classic`, `electric_bike` into `electric` and `docked_bike` into `docked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1605a3",
   "metadata": {},
   "source": [
    "**Q9 Group By**\n",
    "\n",
    "Group the bikes by `rideable_type`, then `member_casual` and **count** the number of `ride_id` in each group.\n",
    "Store the results in a new object called `bikes_group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0ae6d",
   "metadata": {},
   "source": [
    "**Q10 Unstacking Group**\n",
    "\n",
    "Unstack the `bikes_group` group so that the result is a DataFrame of `rideable_type` with columns `casual` and `member`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10 Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ebf15",
   "metadata": {},
   "source": [
    "**Q11 Plot**\n",
    "\n",
    "Create a bar plot using the DataFrame from Q10, with `stacked=True` to create a stacked bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11 Answer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
